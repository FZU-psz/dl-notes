{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面验证 transpose 和 permute 的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3279, -1.0326],\n",
       "         [ 1.2334, -1.0557]],\n",
       "\n",
       "        [[ 0.3534, -0.5765],\n",
       "         [ 0.0330,  0.3341]],\n",
       "\n",
       "        [[-1.3845,  0.4735],\n",
       "         [ 0.6970,  0.5615]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.randn(12).reshape(3,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3279,  1.2334],\n",
       "         [-1.0326, -1.0557]],\n",
       "\n",
       "        [[ 0.3534,  0.0330],\n",
       "         [-0.5765,  0.3341]],\n",
       "\n",
       "        [[-1.3845,  0.6970],\n",
       "         [ 0.4735,  0.5615]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = x.transpose(2,1)\n",
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3279,  1.2334],\n",
       "         [-1.0326, -1.0557]],\n",
       "\n",
       "        [[ 0.3534,  0.0330],\n",
       "         [-0.5765,  0.3341]],\n",
       "\n",
       "        [[-1.3845,  0.6970],\n",
       "         [ 0.4735,  0.5615]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = x.permute(0,2,1)\n",
    "x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头注意力的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "dim_embedding = 16\n",
    "num_heads = 8\n",
    "dim_qk = 16\n",
    "dim_v =16\n",
    "sequence_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(batch_size,sequence_length,dim_embedding)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_q = nn.Linear(dim_embedding,dim_qk, bias=False)\n",
    "linear_k = nn.Linear(dim_embedding,dim_qk, bias=False)\n",
    "linear_v = nn.Linear(dim_embedding,dim_v, bias = False)\n",
    "\n",
    "normal_factor = ( dim_qk // num_heads) ** -0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_q = dim_qk // num_heads\n",
    "div_v = dim_v // num_heads\n",
    "div_q, div_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= linear_q(x).view(batch_size,sequence_length,num_heads, div_q).permute(0,2,1,3)\n",
    "\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k= linear_k(x).view(batch_size,sequence_length,num_heads, div_q).permute(0,2,1,3)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v= linear_v(x).view(batch_size,sequence_length,num_heads,div_v).permute(0,2,1,3)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = torch.matmul(q,k.permute(0,1,3,2)) ** normal_factor\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_socre  = torch.matmul(dist, v) \n",
    "atten_socre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_socre = atten_socre.permute(0,2,1,3)\n",
    "\n",
    "atten_socre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_socre = atten_socre.contiguous().view(batch_size,sequence_length,dim_v)\n",
    "atten_socre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2,2)\n",
    "linear2 = nn.Linear(2,4)\n",
    "model =  nn.Sequential(linear,linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0530, -0.0046],\n",
      "        [-0.5766, -0.7019]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3891, -0.0369], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4832, -0.2867],\n",
      "        [ 0.5267,  0.6571],\n",
      "        [ 0.3882,  0.5172],\n",
      "        [ 0.1580,  0.4386]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5923,  0.0330,  0.5836,  0.6242], requires_grad=True)\n",
      "0 Linear(in_features=2, out_features=2, bias=True)\n",
      "1 Linear(in_features=2, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)\n",
    "for module in list(model.modules()):\n",
    "    for key,val in module._modules.items():\n",
    "        print(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person():\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p = Person('John', 36)\n",
    "\n",
    "setattr(p, 'name', 'John')\n",
    "p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(p, 'sex', 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('female', 36, 'John')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sex,p.age,p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1,2,3]]\n",
    "lst*3\n",
    "## 等价与 [item*3 for item in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (key1,value1),(key2,value2) \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m),(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key1,value1)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfsa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "for (key1,value1),(key2,value2) in [(1,2),(3,4)]:\n",
    "    print(key1,value1)\n",
    "    print(\"dfsa\")\n",
    "    print(key2,value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(,diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k,k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.full((3,3),float(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_socre = torch.matmul(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., -inf, -inf],\n",
       "        [6., 6., -inf],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full((3,3),float(\"-inf\"))\n",
    "mask = torch.triu(mask,diagonal=1)\n",
    "attention_socre = attention_socre + mask\n",
    "attention_socre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(i,j):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i\u001b[38;5;241m+\u001b[39mj\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtime_cal\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m, in \u001b[0;36mtime_cal\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime cost is \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "def time_cal(func):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    func()\n",
    "    end = time.time()\n",
    "    print(\"time cost is \", end-start)\n",
    "    \n",
    "    \n",
    "def add(i,j):\n",
    "    return i+j\n",
    "\n",
    "time_cal(lambda: add(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024263858795166016\n",
      "time cost is  0.025965452194213867\n",
      "time cost is  0.022052288055419922\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "k = torch.ones((3,3))\n",
    "v = torch.full((3,3),float(2))\n",
    "\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    torch.matmul(k,v)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "def two_for_test():\n",
    "    for _ in range(1000):\n",
    "        res = []\n",
    "        for i,k_i in enumerate(k):\n",
    "            line = []\n",
    "            for j,v_i in enumerate(v):\n",
    "                line.append(torch.matmul(k_i,v_i))\n",
    "                if i== j: break\n",
    "            res.append(line)\n",
    "def one_for_test():\n",
    "    for _ in range(1000):\n",
    "        res =  []\n",
    "        for i,k_i in enumerate(k) :\n",
    "            line = torch.matmul(k_i,v[:,:i+1])     \n",
    "            res.append(line)\n",
    "\n",
    "time_cal(two_for_test) \n",
    "time_cal(one_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [1., 2., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_i = torch.tensor([[1,2,1]])\n",
    "\n",
    "torch.cat([v,v_i],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
